import os, json
from sentence_transformers import SentenceTransformer
import chromadb

# ====== C·∫•u h√¨nh ======
DATA_DIR = "./kb/global"        # Th∆∞ m·ª•c ch·ª©a c√°c file JSON t√†i li·ªáu
CHROMA_DIR = "./chroma_db"           # Th∆∞ m·ª•c l∆∞u vector database
COLLECTION_NAME = "global_kb"        # T√™n collection trong Chroma
EMBED_MODEL = os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2")


# ====== Chu·∫©n b·ªã ChromaDB v√† Embedder ======
def get_chroma_collection():
    os.environ.setdefault("ANONYMIZED_TELEMETRY", "false")
    client = chromadb.PersistentClient(path=CHROMA_DIR)
    col = client.get_or_create_collection(COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    return col


def get_embedder():
    os.environ.setdefault("USE_TF", "0")
    return SentenceTransformer(EMBED_MODEL)


# ====== H√†m x·ª≠ l√Ω ======
def build_metadata(doc: dict):
    ctx_tags = doc.get("context_tags", [])
    etypes = doc.get("event_type", [])
    primary = etypes[0].strip() if etypes else ""

    return {
        "event_type_primary": primary,
        "event_type_primary_lower": primary.lower(),  # üëà b·∫Øt bu·ªôc
        "tag_vip": "vip" in ctx_tags,
        "tag_sponsor": "sponsor" in ctx_tags,
        "tag_outdoor": "outdoor" in ctx_tags,
    }
def build_metadata(doc: dict):
    ctx_tags = doc.get("context_tags", [])
    etypes = doc.get("event_type", [])
    primary = etypes[0].strip() if etypes else ""

    return {
        "event_type_primary": primary,
        "event_type_primary_lower": primary.lower(),  # üëà b·∫Øt bu·ªôc
        "tag_vip": "vip" in ctx_tags,
        "tag_sponsor": "sponsor" in ctx_tags,
        "tag_outdoor": "outdoor" in ctx_tags,
    }



def ingest():
    print("üöÄ B·∫Øt ƒë·∫ßu ingest d·ªØ li·ªáu global KB...")
    embedder = get_embedder()
    col = get_chroma_collection()

    all_docs, all_ids, all_metas, all_embs = [], [], [], []

    # ====== ƒê·ªçc t·ª´ng file JSON ======
    for file in os.listdir(DATA_DIR):
        if not file.endswith(".json"):
            continue
        path = os.path.join(DATA_DIR, file)
        with open(path, "r", encoding="utf-8") as f:
            doc = json.load(f)

        doc_id = doc.get("doc_id") or os.path.splitext(file)[0]
        meta = build_metadata(doc)

        # Bi·∫øn t·ª´ng baseline_task th√†nh 1 ƒëo·∫°n vƒÉn ƒë·ªôc l·∫≠p
        tasks = doc.get("baseline_tasks", [])
        text_parts = []
        for t in tasks:
            part = f"{t['name']} ({t['owner_department']}): {t['notes']}"
            text_parts.append(part)
        text_join = "\n".join(text_parts)

        all_ids.append(doc_id)
        all_docs.append(text_join)
        all_metas.append(meta)

    # ====== T·∫°o embeddings ======
    print(f"üî¢ T·∫°o embeddings cho {len(all_docs)} t√†i li·ªáu...")
    all_embs = embedder.encode(all_docs, show_progress_bar=True).tolist()

    # ====== Upsert v√†o Chroma ======
    print(f"üì¶ Upsert v√†o collection '{COLLECTION_NAME}' ...")
    col.upsert(ids=all_ids, documents=all_docs, metadatas=all_metas, embeddings=all_embs)

    print("‚úÖ Ingest ho√†n t·∫•t!")
    print(f"üìÇ T·ªïng c·ªông: {len(all_ids)} t√†i li·ªáu ƒë∆∞·ª£c th√™m v√†o.")


if __name__ == "__main__":
    ingest()
